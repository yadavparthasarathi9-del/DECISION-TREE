{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDcKfQCaHbMr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1**: What is a Decision Tree, and how does it work in the context of classification?\n",
        "- A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It works by splitting data into subsets based on the value of input features, forming a tree-like structure of decision nodes and leaf nodes.\n",
        " - Root Node: The top node that represents the entire dataset.\n",
        "\n",
        "- Decision Nodes: Points where the data is split based on a feature condition.\n",
        "\n",
        "- Leaf Nodes (Terminal Nodes): Represent the final output or class label."
      ],
      "metadata": {
        "id": "CgkIMrDUIc3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2**: Explain the concepts of Gini Impurity and Entropy as impurity measures.How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Gini and Entropy both measure how mixed the classes are in a node.\n",
        "\n",
        "- The Decision Tree algorithm chooses splits that minimize impurity.\n",
        "\n",
        "- Lower impurity → better, more informative split → purer nodes."
      ],
      "metadata": {
        "id": "lroNKQbiJ0cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3**:  What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "-Pre-Pruning stops the tree from growing too deep — fast and efficient.\n",
        "\n",
        "- Post-Pruning trims the fully grown tree — more accurate and generalizable."
      ],
      "metadata": {
        "id": "O6WZhHrNKJsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4**: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "- Information Gain :  how much a feature helps reduce uncertainty.\n",
        "Decision Trees use it to choose the best feature to split on at each step, leading to purer, more informative branches.\n",
        "\n"
      ],
      "metadata": {
        "id": "BHaRPlZRLrnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5**:What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "- Decision Trees are widely used in finance, healthcare, marketing, and manufacturing because they’re interpretable and flexible, but they can overfit easily and are less stable on noisy data.\n",
        "That’s why, in practice, they’re often used as the base model in ensemble methods like Random Forests and Gradient Boosted Trees.\n"
      ],
      "metadata": {
        "id": "d42GTPrnMAIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6**:Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        " ● Train a Decision Tree Classifier using the Gini criterion\n",
        "  ● Print the model’s accuracy and feature importances\n"
      ],
      "metadata": {
        "id": "OmrZF7iFMYpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data          # Features\n",
        "y = iris.target        # Target labels\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Create and train the Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions on the test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Calculate and print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhBtdfUsMzqr",
        "outputId": "144bbd39-8831-4d10-d252-7eab0c71a020"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7**:Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n"
      ],
      "metadata": {
        "id": "xj3gBOGvNAu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data          # Features\n",
        "y = iris.target        # Target labels\n",
        "\n",
        "# 2. Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# 4. Train a fully grown Decision Tree (no depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 5. Compare accuracies\n",
        "print(\"Decision Tree Comparison on Iris Dataset\")\n",
        "print(\"----------------------------------------\")\n",
        "print(f\"Accuracy (max_depth=3): {accuracy_limited * 100:.2f}%\")\n",
        "print(f\"Accuracy (fully-grown): {accuracy_full * 100:.2f}%\")\n",
        "\n",
        "# Optional: show feature importances of the limited tree\n",
        "print(\"\\nFeature Importances (max_depth=3):\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf_limited.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50T44nTJNQJQ",
        "outputId": "19365a80-2194-424c-c50a-4823ff76791e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Comparison on Iris Dataset\n",
            "----------------------------------------\n",
            "Accuracy (max_depth=3): 100.00%\n",
            "Accuracy (fully-grown): 100.00%\n",
            "\n",
            "Feature Importances (max_depth=3):\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.9346\n",
            "petal width (cm): 0.0654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**8**:Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "IxBbJiYxNbzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data           # Features\n",
        "y = housing.target         # Target variable (median house value)\n",
        "\n",
        "# 2. Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(criterion='squared_error', random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv8WHWFANnWd",
        "outputId": "1ebf0191-5420-49da-b71b-a1469350cafc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**9**:Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n"
      ],
      "metadata": {
        "id": "s0arcKbiN3KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # features\n",
        "y = iris.target      # target labels\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get the best parameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# 7. Evaluate on test data\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 8. Print results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "S2JoeJLoOIcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**10**:Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        " - ● Handle the missing values\n",
        "- ● Encode the categorical features\n",
        "- ● Train a Decision Tree model\n",
        "- ● Tune its hyperparameters\n",
        "- ● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "- 1) Understand the data first\n",
        "\n",
        "Inspect data types, value counts, and missingness by column (df.info(), df.describe(), df.isna().mean()).\n",
        "\n",
        "Identify categorical vs numeric features, and flag ordinal variables (e.g., disease stage).\n",
        "\n",
        "Check class balance (value_counts()), outliers, and obvious data quality issues (duplicates, impossible values).\n",
        "\n",
        "Split dataset into train / validation / test before doing any target-related preprocessing to avoid leakage (typical: 60–20–20 or 70–15–15). Use stratified split if classes are imbalanced.\n",
        "\n",
        "- 2) Handle missing values\n",
        "\n",
        "Principles: avoid leaking target info and prefer imputing within cross-validation/pipeline.\n",
        "\n",
        "Options by feature type:\n",
        "\n",
        "Numeric features\n",
        "\n",
        "If missingness is small & MCAR: SimpleImputer(strategy='median') (robust to outliers).\n",
        "\n",
        "If not MCAR or missingness is informative: add a binary indicator column feat_missing and impute (median/mean).\n",
        "\n",
        "For heavy missingness consider dropping the feature or using model-based imputation (IterativeImputer) if justified.\n",
        "\n",
        "Categorical features\n",
        "\n",
        "Treat missing as a separate category (\"MISSING\") or impute with most frequent value; sometimes \"MISSING\" carries signal.\n",
        "\n",
        "Time-dependent / grouped data\n",
        "\n",
        "Impute within groups (e.g., patient ID) if appropriate.\n",
        "\n",
        "Always fit imputers only on training data (use pipelines so this is automatic).\n",
        "\n",
        "- 3) Encode categorical features\n",
        "\n",
        "Decision Trees in common libraries expect numeric input.\n",
        "\n",
        "Nominal categories (no order): use OneHotEncoder(handle_unknown='ignore'). Works well for low-cardinality features.\n",
        "\n",
        "High-cardinality nominal:\n",
        "\n",
        "Target encoding / frequency encoding / embedding approaches. If using target encoding, do it inside cross-validation or use smoothing to prevent leakage.\n",
        "\n",
        "Frequency encoding (replace category by its frequency) is safer and simple.\n",
        "\n",
        "Ordinal categories: use OrdinalEncoder with explicit ordering.\n",
        "\n",
        "If you use sklearn pipelines, combine with ColumnTransformer.\n",
        "\n",
        "Why: Correct encoding preserves relationships and avoids exploding dimensionality.\n",
        "\n",
        "- 5) Train the Decision Tree model\n",
        "\n",
        "Start with sensible defaults (e.g., criterion='gini', class_weight='balanced' if classes unequal).\n",
        "\n",
        "Fit on training data only.\n",
        "\n",
        "Evaluate baseline on validation set.\n",
        "\n",
        "Notes: trees don’t require scaling; scaling shown above is OK but optional.\n",
        "\n",
        "- 6) Tune hyperparameters (GridSearchCV or RandomizedSearchCV)\n",
        "\n",
        "Important hyperparameters for Decision Trees:\n",
        "\n",
        "max_depth (limits tree size)\n",
        "\n",
        "min_samples_split and min_samples_leaf (stability, overfit control)\n",
        "\n",
        "max_leaf_nodes\n",
        "\n",
        "criterion (gini vs entropy)\n",
        "\n",
        "class_weight (or handle imbalance with sampling)\n",
        "\n",
        "Use StratifiedKFold and include the entire pipeline inside the grid search to avoid leakage.\n",
        "\n",
        "- 7) Model evaluation (metrics and plots)\n",
        "\n",
        "Use a held-out test set only once for final evaluation.\n",
        "\n",
        "Recommended metrics for a disease prediction problem:\n",
        "\n",
        "Primary: ROC AUC (probability ranking), PR AUC (precision-recall area) — PR AUC is better when positive class is rare.\n",
        "\n",
        "Threshold-dependent: precision, recall (sensitivity), specificity, F1-score.\n",
        "\n",
        "Confusion matrix — visualize false positives vs false negatives.\n",
        "\n",
        "Calibration: reliability diagram / Brier score — very important in healthcare if probabilities are used in decisions.\n",
        "\n",
        "Decision analysis: compute cost-sensitive metrics if false negatives are much worse than false positives.\n",
        "\n",
        "Confidence intervals: bootstrap metrics to estimate uncertainty."
      ],
      "metadata": {
        "id": "Ov9JPT4XOz3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "num_cols = [...]\n",
        "cat_lowcard = [...]\n",
        "cat_highcard = [...]\n",
        "ord_cols = [...]\n",
        "\n",
        "num_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())   # optional for trees but OK to keep\n",
        "])\n",
        "\n",
        "cat_low_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "cat_high_pipe = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
        "    # use frequency or leave as string to custom encoder\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipe, num_cols),\n",
        "    ('cat_low', cat_low_pipe, cat_lowcard),\n",
        "    # add others...\n",
        "])\n",
        "\n",
        "clf = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('model', DecisionTreeClassifier(random_state=42, class_weight='balanced'))\n",
        "])\n"
      ],
      "metadata": {
        "id": "rfoVquYwO7af"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}